{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b223a8",
   "metadata": {},
   "source": [
    "# Section 0: Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33c052e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import section\n",
    "\n",
    "# Helper funcitons and classes\n",
    "from helper_func_and_classes import create_dataset_list, create_submission_file, create_vocab\n",
    "from helper_func_and_classes import split_dataset\n",
    "from helper_func_and_classes import word_vec_to_word_embeddings\n",
    "from helper_func_and_classes import word_vec_to_aggregated_word_embeddings\n",
    "from helper_func_and_classes import word_embeddings_extraction\n",
    "from helper_func_and_classes import TwitterDataset\n",
    "from helper_func_and_classes import get_count_of_longest_sentence\n",
    "\n",
    "# word embeddings\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# random state\n",
    "RANDOM_SEED = 123 # used in helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88567cf5",
   "metadata": {},
   "source": [
    "# Section 1: Data preprocessing section\n",
    "This section will extract the data from the three different .txt files. Then, the helper functions will process the tweets to create our vocabulary and three python lists for positive, negative, and submission tweets. The tweets are pre tokenized, so we only split on white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b8ed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1250000/1250000 [00:01<00:00, 825122.96it/s]\n",
      "100%|█████████████████████████████████████| 1250000/1250000 [00:01<00:00, 716216.56it/s]\n",
      "100%|█████████████████████████████████████████| 10000/10000 [00:00<00:00, 672314.94it/s]\n",
      "100%|███████████████████████████████████████| 100000/100000 [00:00<00:00, 949057.80it/s]\n",
      "100%|███████████████████████████████████████| 100000/100000 [00:00<00:00, 893423.99it/s]\n",
      "100%|█████████████████████████████████████████| 10000/10000 [00:00<00:00, 934393.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text_vocab_full:  604014\n",
      "Length of text_vocab_lite:  127802\n",
      "Length of submission_data:  10000\n"
     ]
    }
   ],
   "source": [
    "# creating the text vocabulary from the whole data set, including positive, negative and test data.\n",
    "text_vocab_full = create_vocab(\"./twitter-datasets/train_pos_full.txt\",\n",
    "                          \"./twitter-datasets/train_neg_full.txt\",\n",
    "                          \"./twitter-datasets/test_data.txt\")\n",
    "\n",
    "text_vocab_lite = create_vocab(\"./twitter-datasets/train_pos.txt\",\n",
    "                          \"./twitter-datasets/train_neg.txt\",\n",
    "                          \"./twitter-datasets/test_data.txt\")\n",
    "\n",
    "# creating a standard python library list of the tweets that will be used for submission, 1 tweet per index\n",
    "submission_data = create_dataset_list(\"./twitter-datasets/test_data.txt\")\n",
    "\n",
    "print(\"Length of text_vocab_full: \", len(text_vocab_full))\n",
    "print(\"Length of text_vocab_lite: \", len(text_vocab_lite))\n",
    "print(\"Length of submission_data: \",len(submission_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f695de6d",
   "metadata": {},
   "source": [
    "Here we will create the TwitterDataset class which contains all the pre processed data, we have done a sentence cutoff for the sentences longe than 37, this was found in the exploratory data analysis part.  \n",
    "\n",
    "Furthermore we will split and shuffle the dataset into training and testing. The longest sentences is also located by using the get_count_of_longest_sentence on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5fde58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1250000/1250000 [00:07<00:00, 171408.23it/s]\n",
      "100%|█████████████████████████████████████| 1250000/1250000 [00:09<00:00, 138731.53it/s]\n",
      "100%|███████████████████████████████████████| 100000/100000 [00:00<00:00, 237080.85it/s]\n",
      "100%|███████████████████████████████████████| 100000/100000 [00:00<00:00, 206120.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in train_data is:  2246870\n",
      "Number of elements in test_data is:  249653\n",
      "Number of elements in train_data is:  179784\n",
      "Number of elements in test_data is:  19976\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset in the TwitterDataset class which is found in the helper functions\n",
    "dataset_full = TwitterDataset(text_vocab_full,\n",
    "                              create_dataset_list(\"./twitter-datasets/train_pos_full.txt\"),\n",
    "                              create_dataset_list(\"./twitter-datasets/train_neg_full.txt\"),\n",
    "                              submission_data,\n",
    "                              37)\n",
    "\n",
    "dataset_lite = TwitterDataset(text_vocab_lite, \n",
    "                              create_dataset_list(\"./twitter-datasets/train_pos.txt\"),\n",
    "                              create_dataset_list(\"./twitter-datasets/train_neg.txt\"),\n",
    "                              submission_data,\n",
    "                              37)\n",
    "\n",
    "# create training dataset and test dataset - using a split of 85% / 15%\n",
    "train_dataset_full, test_dataset_full = split_dataset(dataset_full, 0.9);\n",
    "train_dataset_lite, test_dataset_lite = split_dataset(dataset_lite, 0.9);\n",
    "\n",
    "# calculating the longest sentence\n",
    "max_len_full = get_count_of_longest_sentence(dataset_full)\n",
    "max_len_lite = get_count_of_longest_sentence(dataset_lite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e59d62",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Section 2: RandomForestClassifier\n",
    "#### Pretrained word embeddings (dim=200) \n",
    "Here we will use pre-trained Global Vector word embeddings (Glove); these will have a dimension of 200 per word. The pre-trained word embeddings were downloaded from https://pytorch.org/text/stable/_modules/torchtext/vocab/vectors.html#GloVe. The word embedding used has been trained on Twitter data, which will increase accuracy since our corpus also consists of Twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "050275b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vec = GloVe(name='twitter.27B', dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1efbd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_lite = glove_vec.get_vecs_by_tokens(list(text_vocab_lite.keys()), lower_case_backup=True)\n",
    "word_embeddings_full = glove_vec.get_vecs_by_tokens(list(text_vocab_full.keys()), lower_case_backup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec384b",
   "metadata": {},
   "source": [
    "We will create the word embeddings for our train data and test data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f174bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "179784it [00:08, 20649.89it/s]\n",
      "19976it [00:00, 21709.10it/s]\n",
      "2246870it [01:46, 21020.64it/s]\n",
      "249653it [00:11, 21194.14it/s]\n"
     ]
    }
   ],
   "source": [
    "dimension = 200\n",
    "vec_size_lite = max_len_lite\n",
    "vec_size_full = max_len_full\n",
    "\n",
    "train_matrix_lite, train_labels_lite = word_vec_to_aggregated_word_embeddings(\n",
    "    train_dataset_lite,\n",
    "    word_embeddings_lite,\n",
    "    dimension)\n",
    "\n",
    "test_matrix_lite, test_labels_lite = word_vec_to_aggregated_word_embeddings(\n",
    "    test_dataset_lite,\n",
    "    word_embeddings_lite,\n",
    "    dimension)\n",
    "\n",
    "train_matrix_full, train_labels_full = word_vec_to_aggregated_word_embeddings(\n",
    "    train_dataset_full,\n",
    "    word_embeddings_full,\n",
    "    dimension)\n",
    "\n",
    "test_matrix_full, test_labels_full = word_vec_to_aggregated_word_embeddings(\n",
    "    test_dataset_full,\n",
    "    word_embeddings_full,\n",
    "    dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba1608",
   "metadata": {},
   "source": [
    "## Section 2.1: Choosing best parameters\n",
    "Here we will optimize the parameters for the `RandomForestClassifier`. \n",
    "\n",
    "We will do a randomized search for parameters because it is a huge amount of parameters to optimize in the `RandomForestClassifer`.  \n",
    "Therefore after creating the grid of parameters, we will use the `RandomizedSearchCV` to collect a random collection of parameters form the `grid_params`. To ensure reproducability we will change the cross-validation function to `StratifiedKFolk`, this way we can set the `random-state` to our `RANDOM_SEED` constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b28671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random grid\n",
    "grid_params = {\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'n_estimators': [5, 10, 20, 30, 40],\n",
    "    'min_samples_leaf': [1, 2, 3, 4],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'max_depth': [10, 50, 100, 150, 200, 250, 300],\n",
    "    'bootstrap': [False, True]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c260b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 150 candidates, totalling 450 fits\n",
      "CPU times: user 3min 38s, sys: 1min 11s, total: 4min 50s\n",
      "Wall time: 53min 56s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=3, random_state=123, shuffle=True),\n",
       "                   estimator=RandomForestClassifier(random_state=123),\n",
       "                   n_iter=150, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [False, True],\n",
       "                                        'max_depth': [10, 50, 100, 150, 200,\n",
       "                                                      250, 300],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 3, 4],\n",
       "                                        'min_samples_split': [2, 5, 10, 15, 20],\n",
       "                                        'n_estimators': [5, 10, 20, 30, 40]},\n",
       "                   random_state=123, verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv = StratifiedKFold(n_splits=3, random_state=RANDOM_SEED, shuffle=True)\n",
    "rf_clf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "\n",
    "rf_random_clf = RandomizedSearchCV(\n",
    "    param_distributions = grid_params,\n",
    "    estimator = rf_clf,\n",
    "    n_iter = 150,\n",
    "    cv = cv,\n",
    "    verbose=1,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs = -1)\n",
    "\n",
    "rf_random_clf.fit(train_matrix_lite, train_labels_lite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b99ff70",
   "metadata": {},
   "source": [
    "After this we can collect the best parameters by using `best_params_`. We will use these values to train the model on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dd7120a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 40,\n",
       " 'min_samples_split': 15,\n",
       " 'min_samples_leaf': 3,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 100,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64f6304",
   "metadata": {},
   "source": [
    "`{'n_estimators': 40,\n",
    " 'min_samples_split': 15,\n",
    " 'min_samples_leaf': 3,\n",
    " 'max_features': 'sqrt',\n",
    " 'max_depth': 100,\n",
    " 'bootstrap': False}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec75c33",
   "metadata": {},
   "source": [
    "## Section 2.2: Training the RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287f7aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 40building tree 2 of 40\n",
      "building tree 3 of 40\n",
      "\n",
      "building tree 4 of 40building tree 5 of 40\n",
      "\n",
      "building tree 6 of 40\n",
      "building tree 7 of 40building tree 8 of 40\n",
      "building tree 9 of 40\n",
      "\n",
      "building tree 10 of 40building tree 11 of 40\n",
      "building tree 12 of 40\n",
      "\n",
      "building tree 13 of 40\n",
      "building tree 14 of 40\n",
      "building tree 15 of 40\n",
      "building tree 16 of 40\n",
      "building tree 17 of 40\n",
      "building tree 18 of 40\n",
      "building tree 19 of 40\n",
      "building tree 20 of 40\n",
      "building tree 21 of 40\n",
      "building tree 22 of 40\n",
      "building tree 23 of 40\n",
      "building tree 24 of 40\n",
      "building tree 25 of 40\n",
      "building tree 26 of 40\n",
      "building tree 27 of 40\n",
      "building tree 28 of 40\n",
      "building tree 29 of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  6.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 30 of 40\n",
      "building tree 31 of 40\n",
      "building tree 32 of 40\n",
      "building tree 33 of 40\n",
      "building tree 34 of 40\n",
      "building tree 35 of 40\n",
      "building tree 36 of 40\n",
      "building tree 37 of 40\n",
      "building tree 38 of 40\n",
      "building tree 39 of 40\n",
      "building tree 40 of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  38 out of  40 | elapsed: 11.0min remaining:   34.9s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed: 11.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_depth=100, max_features='sqrt',\n",
       "                       min_samples_leaf=3, min_samples_split=15,\n",
       "                       n_estimators=40, n_jobs=-1, random_state=123, verbose=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_best_params = RandomForestClassifier(\n",
    "    n_estimators=40, \n",
    "    min_samples_split=15, \n",
    "    min_samples_leaf=3,\n",
    "    max_features='sqrt',\n",
    "    max_depth=100,\n",
    "    bootstrap=False,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "rf_clf_best_params.fit(train_matrix_full, train_labels_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04263e34",
   "metadata": {},
   "source": [
    "Then we do a prediction on the test data that we had put aside and use the `classification_report` to see how well it did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41073350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=12)]: Done  38 out of  40 | elapsed:    1.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done  40 out of  40 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8300    0.7478    0.7867    124773\n",
      "         1.0     0.7707    0.8469    0.8070    124880\n",
      "\n",
      "    accuracy                         0.7974    249653\n",
      "   macro avg     0.8003    0.7974    0.7969    249653\n",
      "weighted avg     0.8003    0.7974    0.7969    249653\n",
      "\n",
      "[0. 0. 0. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "label_prediction = rf_clf_best_params.predict(test_matrix_full)\n",
    "print(classification_report(test_labels_full, label_prediction, digits=4))\n",
    "print(label_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4888be",
   "metadata": {},
   "source": [
    "# Section 3: Creating submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea0722e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:01, 8067.42it/s]\n"
     ]
    }
   ],
   "source": [
    "submission_matrix, id_submission_matrix = word_vec_to_aggregated_word_embeddings(\n",
    "    dataset_full.submission_dataset,\n",
    "    word_embeddings_full,\n",
    "    dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a1936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done  38 out of  40 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done  40 out of  40 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_numpy_array = rf_clf_best_params.predict(submission_matrix)\n",
    "submission_numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4484f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission_file(submission_numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a78113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
