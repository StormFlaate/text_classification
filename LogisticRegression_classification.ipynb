{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f069d03",
   "metadata": {},
   "source": [
    "# Section 0: Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955104c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import section\n",
    "\n",
    "# Helper funcitons and classes\n",
    "from helper_func_and_classes import create_dataset_list, create_submission_file, create_vocab\n",
    "from helper_func_and_classes import split_dataset\n",
    "from helper_func_and_classes import word_vec_to_word_embeddings\n",
    "from helper_func_and_classes import word_vec_to_aggregated_word_embeddings\n",
    "from helper_func_and_classes import word_embeddings_extraction\n",
    "from helper_func_and_classes import TwitterDataset\n",
    "from helper_func_and_classes import get_count_of_longest_sentence\n",
    "\n",
    "# word embeddings\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# random state\n",
    "RANDOM_SEED = 123 # used in helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88567cf5",
   "metadata": {},
   "source": [
    "# Section 1: Data preprocessing section\n",
    "This section will extract the data from the three different .txt files. Then, the helper functions will process the tweets to create our vocabulary and three python lists for positive, negative, and submission tweets. The tweets are pre tokenized, so we only split on white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "401f41c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 1250000/1250000 [00:01<00:00, 800910.95it/s]\n",
      "100%|████████████████████████████████████████████████████| 1250000/1250000 [00:01<00:00, 695402.22it/s]\n",
      "100%|████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 666450.15it/s]\n",
      "100%|██████████████████████████████████████████████████████| 100000/100000 [00:00<00:00, 880505.17it/s]\n",
      "100%|██████████████████████████████████████████████████████| 100000/100000 [00:00<00:00, 817314.75it/s]\n",
      "100%|████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 868655.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text_vocab_full:  604014\n",
      "Length of text_vocab_lite:  127802\n",
      "Length of submission_data:  10000\n"
     ]
    }
   ],
   "source": [
    "# creating the text vocabulary from the whole data set, including positive, negative and test data.\n",
    "text_vocab_full = create_vocab(\"./twitter-datasets/train_pos_full.txt\",\n",
    "                          \"./twitter-datasets/train_neg_full.txt\",\n",
    "                          \"./twitter-datasets/test_data.txt\")\n",
    "\n",
    "text_vocab_lite = create_vocab(\"./twitter-datasets/train_pos.txt\",\n",
    "                          \"./twitter-datasets/train_neg.txt\",\n",
    "                          \"./twitter-datasets/test_data.txt\")\n",
    "\n",
    "# creating a standard python library list of the tweets that will be used for submission, 1 tweet per index\n",
    "submission_data = create_dataset_list(\"./twitter-datasets/test_data.txt\")\n",
    "\n",
    "print(\"Length of text_vocab_full: \", len(text_vocab_full))\n",
    "print(\"Length of text_vocab_lite: \", len(text_vocab_lite))\n",
    "print(\"Length of submission_data: \",len(submission_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f695de6d",
   "metadata": {},
   "source": [
    "Here we will create the TwitterDataset class which contains all the pre processed data, here we do not have any cutoff since we can have any length for the quotes since they are just aggregated over the same dimensions.\n",
    "\n",
    "Furthermore we will split and shuffle the dataset into training and testing. The longest sentences is also located by using the get_count_of_longest_sentence on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f3d329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 1250000/1250000 [00:08<00:00, 152301.73it/s]\n",
      "100%|████████████████████████████████████████████████████| 1250000/1250000 [00:10<00:00, 118618.90it/s]\n",
      "100%|██████████████████████████████████████████████████████| 100000/100000 [00:00<00:00, 225059.53it/s]\n",
      "100%|██████████████████████████████████████████████████████| 100000/100000 [00:00<00:00, 184720.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in train_data is:  2250000\n",
      "Number of elements in test_data is:  250000\n",
      "Number of elements in train_data is:  180000\n",
      "Number of elements in test_data is:  20000\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset in the TwitterDataset class which is found in the helper functions\n",
    "dataset_full = TwitterDataset(text_vocab_full,\n",
    "                              create_dataset_list(\"./twitter-datasets/train_pos_full.txt\"),\n",
    "                              create_dataset_list(\"./twitter-datasets/train_neg_full.txt\"),\n",
    "                              submission_data,\n",
    "                              1000)\n",
    "\n",
    "dataset_lite = TwitterDataset(text_vocab_lite, \n",
    "                              create_dataset_list(\"./twitter-datasets/train_pos.txt\"),\n",
    "                              create_dataset_list(\"./twitter-datasets/train_neg.txt\"),\n",
    "                              submission_data,\n",
    "                              1000)\n",
    "\n",
    "# create training dataset and test dataset - using a split of 85% / 15%\n",
    "train_dataset_full, test_dataset_full = split_dataset(dataset_full, 0.9);\n",
    "train_dataset_lite, test_dataset_lite = split_dataset(dataset_lite, 0.9);\n",
    "\n",
    "# calculating the longest sentence\n",
    "max_len_full = get_count_of_longest_sentence(dataset_full)\n",
    "max_len_lite = get_count_of_longest_sentence(dataset_lite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b60c08",
   "metadata": {},
   "source": [
    "# Section 2: Logistic regression\n",
    "#### Pretrained word embeddings (dim=200) \n",
    "Here we will use pre-trained Global Vector word embeddings (Glove); these will have a dimension of 200 per word. The pre-trained word embeddings were downloaded from https://pytorch.org/text/stable/_modules/torchtext/vocab/vectors.html#GloVe. The word embedding used has been trained on Twitter data, which will increase accuracy since our corpus also consists of Twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "050275b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vec = GloVe(name='twitter.27B', dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1efbd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_lite = glove_vec.get_vecs_by_tokens(list(text_vocab_lite.keys()), lower_case_backup=True)\n",
    "word_embeddings_full = glove_vec.get_vecs_by_tokens(list(text_vocab_full.keys()), lower_case_backup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eff6b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2250000it [01:54, 19624.10it/s]\n",
      "250000it [00:15, 15919.97it/s]\n",
      "180000it [00:10, 17919.57it/s]\n",
      "20000it [00:00, 20660.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# full version of dataset used for training the model\n",
    "matrix_train_full, labels_train_full = word_vec_to_aggregated_word_embeddings(train_dataset_full, word_embeddings_full, 200)\n",
    "scaled_matrix_train_full = scale(matrix_train_full)\n",
    "\n",
    "matrix_test_full, labels_test_full = word_vec_to_aggregated_word_embeddings(test_dataset_full, word_embeddings_full, 200)\n",
    "scaled_matrix_test_full = scale(matrix_test_full)\n",
    "\n",
    "# small version of dataset used for param optimizing\n",
    "matrix_train_lite, labels_train_lite = word_vec_to_aggregated_word_embeddings(train_dataset_lite, word_embeddings_lite, 200)\n",
    "scaled_matrix_train_lite = scale(matrix_train_lite)\n",
    "\n",
    "matrix_test_lite, labels_test_lite = word_vec_to_aggregated_word_embeddings(test_dataset_lite, word_embeddings_lite, 200)\n",
    "scaled_matrix_test_lite = scale(matrix_test_lite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4a2d1",
   "metadata": {},
   "source": [
    "## Section 2.1: Choosing best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1989168",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_params = {\n",
    "    'solver': ['newton-cg', 'sag', 'saga'],\n",
    "    'penalty': ['l2'],\n",
    "    'C': [1e5, 1e4, 1e3, 1e2, 1e1, 1e-1, 1e-2, 1e-3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc2fc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "CPU times: user 25.2 s, sys: 20.8 s, total: 46 s\n",
      "Wall time: 2min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LogisticRegression(random_state=123), n_jobs=-1,\n",
       "             param_grid={'C': [100000.0, 10000.0, 1000.0, 100.0, 10.0, 0.1,\n",
       "                               0.01, 0.001],\n",
       "                         'penalty': ['l2'],\n",
       "                         'solver': ['newton-cg', 'sag', 'saga']},\n",
       "             return_train_score=True, verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_clf = LogisticRegression(random_state=RANDOM_SEED)\n",
    "param_search = GridSearchCV(\n",
    "    estimator=lr_clf,\n",
    "    param_grid=grid_params,\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "param_search.fit(matrix_train_lite, labels_train_lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec1ddf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10.0, 'penalty': 'l2', 'solver': 'sag'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00a711",
   "metadata": {},
   "source": [
    "**Best parameters form the parameters grid search:**  \n",
    "`{'C': 1000.0, 'penalty': 'l2', 'solver': 'sag'}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfec4e",
   "metadata": {},
   "source": [
    "## Section 2.2: Training the Logistic regression\n",
    "Since we have figured out the best parameters in the last step, we will used them here. We also set verbose=1 so we can track the progrss of the model.\n",
    "\n",
    "We also set `dual=False` since we have n_samples > n_features, as explained in the documentation found at https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97631209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 21 epochs took 85 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000, n_jobs=-1, random_state=123, solver='sag', verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf_best_params = LogisticRegression(\n",
    "    random_state=RANDOM_SEED,\n",
    "    solver='sag',\n",
    "    C=1000,\n",
    "    penalty='l2',\n",
    "    n_jobs=-1,\n",
    "    fit_intercept=True,\n",
    "    dual=False,\n",
    "    verbose=1\n",
    "    )\n",
    "lr_clf_best_params.fit(scaled_matrix_train_full, labels_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b5a131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7939    0.7594    0.7763    124929\n",
      "         1.0     0.7697    0.8031    0.7860    125071\n",
      "\n",
      "    accuracy                         0.7813    250000\n",
      "   macro avg     0.7818    0.7813    0.7812    250000\n",
      "weighted avg     0.7818    0.7813    0.7812    250000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_prediction = lr_clf_best_params.predict(scaled_matrix_test_full)\n",
    "print(classification_report(labels_test_full, label_prediction, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac9d56",
   "metadata": {},
   "source": [
    "# Section 3: Creating submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_matrix, id_submission_matrix = word_vec_to_aggregated_word_embeddings(\n",
    "    dataset_full.submission_dataset,\n",
    "    word_embeddings_full,\n",
    "    200)\n",
    "scaled_submission_matrix = scale(submission_matrix)\n",
    "\n",
    "label_prediction_submission = lr_clf_best_params.predict(submission_matrix)\n",
    "create_submission_file(label_prediction_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb7f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
